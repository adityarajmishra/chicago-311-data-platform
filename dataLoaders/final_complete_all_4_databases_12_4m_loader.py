#!/usr/bin/env python3
"""
FINAL COMPLETE ALL 4 DATABASES 12.4M LOADER
Fixes Elasticsearch mapping and gets ALL 4 databases working with full 12.4M records
"""

import os
import sys
import time
import logging
import requests
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import traceback

# Add src to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from databases.mongodb_handler import MongoDBHandler
from databases.elasticsearch_handler import ElasticsearchHandler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FinalCompleteAll4DatabasesLoader:
    """FINAL loader that gets ALL 4 databases working with complete 12.4M records."""
    
    def __init__(self):
        self.api_url = "https://data.cityofchicago.org/resource/v6vf-nfxy.json"
        self.batch_size = 5000
        self.max_retries = 3
        self.delay_between_batches = 0.5
        
        # Database handlers
        self.handlers = {}
        
        # Progress tracking
        self.total_processed = 0
        self.start_time = None
        
    def setup_databases(self):
        """Setup all 4 databases with FINAL COMPLETE FIXES."""
        logger.info("ğŸ”§ FINAL SETUP - ALL 4 databases with COMPLETE FIXES...")
        
        # 1. MongoDB (working)
        try:
            logger.info("Setting up MongoDB...")
            mongo_handler = MongoDBHandler()
            mongo_handler.collection.delete_many({})
            self.handlers['MongoDB'] = mongo_handler
            logger.info("âœ… MongoDB ready")
        except Exception as e:
            logger.error(f"âŒ MongoDB setup failed: {e}")
            
        # 2. Elasticsearch with COMPLETE MAPPING FIX
        try:
            logger.info("Setting up Elasticsearch with COMPLETE mapping fix...")
            import requests
            
            class FixedElasticsearchHandler:
                def __init__(self):
                    from elasticsearch import Elasticsearch
                    self.es = Elasticsearch(['http://localhost:9200'])
                    self.index_name = "chicago_311_fixed"
                    self.create_proper_index()
                
                def create_proper_index(self):
                    # Delete existing index
                    try:
                        self.es.indices.delete(index=self.index_name, ignore=[400, 404])
                    except:
                        pass
                    
                    # Create index with flexible mapping
                    mapping = {
                        "mappings": {
                            "dynamic": True,
                            "properties": {
                                "sr_number": {"type": "keyword"},
                                "sr_type": {
                                    "type": "text",
                                    "fields": {"keyword": {"type": "keyword"}}
                                },
                                "sr_short_code": {"type": "keyword"},
                                "owner_department": {
                                    "type": "text", 
                                    "fields": {"keyword": {"type": "keyword"}}
                                },
                                "status": {"type": "keyword"},
                                "created_date": {"type": "date"},
                                "last_modified_date": {"type": "date"},
                                "closed_date": {"type": "date"},
                                "completion_date": {"type": "date"},
                                "due_date": {"type": "date"},
                                "street_address": {
                                    "type": "text",
                                    "fields": {"keyword": {"type": "keyword"}}
                                },
                                "city": {"type": "keyword"},
                                "state": {"type": "keyword"},
                                "zip_code": {"type": "keyword"},
                                "community_area": {"type": "integer"},
                                "ward": {"type": "integer"},
                                "latitude": {"type": "float"},
                                "longitude": {"type": "float"},
                                "location_point": {"type": "geo_point"},
                                "origin": {"type": "keyword"},
                                "type_of_service_request": {
                                    "type": "text",
                                    "fields": {"keyword": {"type": "keyword"}}
                                },
                                "most_recent_action": {
                                    "type": "text",
                                    "fields": {"keyword": {"type": "keyword"}}
                                },
                                "current_activity": {
                                    "type": "text",
                                    "fields": {"keyword": {"type": "keyword"}}
                                }
                            }
                        }
                    }
                    
                    self.es.indices.create(index=self.index_name, body=mapping)
                    logger.info(f"âœ… Created Elasticsearch index: {self.index_name}")
            
            es_handler = FixedElasticsearchHandler()
            self.handlers['Elasticsearch'] = es_handler
            logger.info("âœ… Elasticsearch ready")
        except Exception as e:
            logger.error(f"âŒ Elasticsearch setup failed: {e}")
            
        # 3. PostgreSQL (working)
        try:
            logger.info("Setting up PostgreSQL...")
            import psycopg2
            
            class WorkingPostgreSQLHandler:
                def __init__(self):
                    try:
                        self.connection = psycopg2.connect(
                            host="localhost",
                            port=5432,
                            database="postgres",
                            user="postgres",
                            password="postgres"
                        )
                        self.connection.autocommit = True
                    except:
                        os.system("createdb chicago_311 2>/dev/null || true")
                        self.connection = psycopg2.connect(
                            host="localhost",
                            port=5432,
                            database="chicago_311",
                            user="postgres",
                            password=""
                        )
                        self.connection.autocommit = True
                    
                    self.create_table()
                
                def create_table(self):
                    create_sql = """
                    DROP TABLE IF EXISTS chicago_311_requests;
                    CREATE TABLE chicago_311_requests (
                        id SERIAL PRIMARY KEY,
                        sr_number VARCHAR(50) UNIQUE,
                        sr_type VARCHAR(500),
                        sr_short_code VARCHAR(50),
                        owner_department VARCHAR(200),
                        status VARCHAR(50),
                        created_date TIMESTAMP,
                        last_modified_date TIMESTAMP,
                        closed_date TIMESTAMP,
                        street_address VARCHAR(500),
                        city VARCHAR(100),
                        state VARCHAR(20),
                        zip_code VARCHAR(50),
                        street_number VARCHAR(50),
                        street_direction VARCHAR(50),
                        street_name VARCHAR(300),
                        street_type VARCHAR(100),
                        duplicate_ssr VARCHAR(50),
                        legacy_sr_number VARCHAR(50),
                        legacy_record VARCHAR(50),
                        parent_sr_number VARCHAR(50),
                        community_area INTEGER,
                        ward INTEGER,
                        electrical_district VARCHAR(200),
                        electricity_grid VARCHAR(200),
                        police_sector VARCHAR(200),
                        police_district VARCHAR(200),
                        police_beat VARCHAR(200),
                        precinct VARCHAR(200),
                        sanitation_division_days VARCHAR(200),
                        created_hour INTEGER,
                        created_day_of_week VARCHAR(50),
                        created_month INTEGER,
                        x_coordinate DOUBLE PRECISION,
                        y_coordinate DOUBLE PRECISION,
                        latitude DOUBLE PRECISION,
                        longitude DOUBLE PRECISION,
                        location_point TEXT,
                        origin VARCHAR(200),
                        type_of_service_request VARCHAR(500),
                        most_recent_action VARCHAR(500),
                        current_activity VARCHAR(500),
                        number_of_days_pending INTEGER,
                        completion_date TIMESTAMP,
                        due_date TIMESTAMP,
                        ssa VARCHAR(50),
                        channel VARCHAR(100),
                        historical_wards_03_15 INTEGER,
                        zip_codes INTEGER,
                        community_areas INTEGER,
                        census_tracts BIGINT,
                        wards INTEGER
                    );
                    CREATE INDEX IF NOT EXISTS idx_sr_number ON chicago_311_requests(sr_number);
                    """
                    
                    with self.connection.cursor() as cursor:
                        cursor.execute(create_sql)
                
                def bulk_insert(self, data):
                    if not data:
                        return 0
                    
                    clean_data = []
                    for record in data:
                        clean_record = {}
                        for key, value in record.items():
                            if key == '_id':
                                continue
                            if isinstance(value, str) and len(value) > 500:
                                clean_record[key] = value[:500]
                            elif value is None or str(value) in ['', 'nan', 'None']:
                                clean_record[key] = None
                            else:
                                clean_record[key] = value
                        clean_data.append(clean_record)
                    
                    if not clean_data:
                        return 0
                    
                    available_columns = list(clean_data[0].keys())
                    table_columns = [
                        'sr_number', 'sr_type', 'sr_short_code', 'owner_department', 'status',
                        'created_date', 'last_modified_date', 'closed_date', 'street_address',
                        'city', 'state', 'zip_code', 'street_number', 'street_direction',
                        'street_name', 'street_type', 'duplicate_ssr', 'legacy_sr_number',
                        'legacy_record', 'parent_sr_number', 'community_area', 'ward',
                        'electrical_district', 'electricity_grid', 'police_sector',
                        'police_district', 'police_beat', 'precinct',
                        'sanitation_division_days', 'created_hour', 'created_day_of_week',
                        'created_month', 'x_coordinate', 'y_coordinate', 'latitude',
                        'longitude', 'location_point', 'origin', 'type_of_service_request',
                        'most_recent_action', 'current_activity', 'number_of_days_pending',
                        'completion_date', 'due_date', 'ssa', 'channel',
                        'historical_wards_03_15', 'zip_codes', 'community_areas',
                        'census_tracts', 'wards'
                    ]
                    
                    insert_columns = [col for col in table_columns if col in available_columns]
                    placeholders = ', '.join(['%s'] * len(insert_columns))
                    columns_str = ', '.join(insert_columns)
                    
                    insert_sql = f"""
                    INSERT INTO chicago_311_requests ({columns_str}) 
                    VALUES ({placeholders})
                    ON CONFLICT (sr_number) DO NOTHING
                    """
                    
                    values = []
                    for record in clean_data:
                        row_values = []
                        for col in insert_columns:
                            value = record.get(col)
                            # Convert dict/list to JSON string for PostgreSQL
                            if isinstance(value, (dict, list)):
                                import json
                                value = json.dumps(value)
                            row_values.append(value)
                        values.append(tuple(row_values))
                    
                    try:
                        with self.connection.cursor() as cursor:
                            cursor.executemany(insert_sql, values)
                            return cursor.rowcount
                    except Exception as e:
                        logger.error(f"PostgreSQL insert error: {e}")
                        return 0
            
            pg_handler = WorkingPostgreSQLHandler()
            self.handlers['PostgreSQL'] = pg_handler
            logger.info("âœ… PostgreSQL ready")
        except Exception as e:
            logger.error(f"âŒ PostgreSQL setup failed: {e}")
            
        # 4. DuckDB (working)
        try:
            logger.info("Setting up DuckDB...")
            import duckdb
            
            class WorkingDuckDBHandler:
                def __init__(self):
                    self.db_path = "chicago_311.duckdb"
                    self.conn = duckdb.connect(self.db_path)
                    self.create_table()
                
                def create_table(self):
                    create_sql = """
                    DROP TABLE IF EXISTS chicago_311_requests;
                    CREATE TABLE chicago_311_requests (
                        sr_number VARCHAR,
                        sr_type VARCHAR,
                        sr_short_code VARCHAR,
                        owner_department VARCHAR,
                        status VARCHAR,
                        created_date VARCHAR,
                        last_modified_date VARCHAR,
                        closed_date VARCHAR,
                        street_address VARCHAR,
                        city VARCHAR,
                        state VARCHAR,
                        zip_code VARCHAR,
                        street_number VARCHAR,
                        street_direction VARCHAR,
                        street_name VARCHAR,
                        street_type VARCHAR,
                        duplicate_ssr VARCHAR,
                        legacy_sr_number VARCHAR,
                        legacy_record VARCHAR,
                        parent_sr_number VARCHAR,
                        community_area VARCHAR,
                        ward VARCHAR,
                        electrical_district VARCHAR,
                        electricity_grid VARCHAR,
                        police_sector VARCHAR,
                        police_district VARCHAR,
                        police_beat VARCHAR,
                        precinct VARCHAR,
                        sanitation_division_days VARCHAR,
                        created_hour VARCHAR,
                        created_day_of_week VARCHAR,
                        created_month VARCHAR,
                        x_coordinate VARCHAR,
                        y_coordinate VARCHAR,
                        latitude VARCHAR,
                        longitude VARCHAR,
                        location_point VARCHAR,
                        origin VARCHAR,
                        type_of_service_request VARCHAR,
                        most_recent_action VARCHAR,
                        current_activity VARCHAR,
                        number_of_days_pending VARCHAR,
                        completion_date VARCHAR,
                        due_date VARCHAR,
                        ssa VARCHAR,
                        channel VARCHAR,
                        historical_wards_03_15 VARCHAR,
                        zip_codes VARCHAR,
                        community_areas VARCHAR,
                        census_tracts VARCHAR,
                        wards VARCHAR
                    );
                    """
                    self.conn.execute(create_sql)
                
                def bulk_insert(self, data):
                    if not data:
                        return 0
                    
                    # Convert everything to strings
                    clean_data = []
                    for record in data:
                        clean_record = {}
                        for key, value in record.items():
                            if key == '_id':
                                continue
                            if value is None or str(value) == 'nan':
                                clean_record[key] = None
                            else:
                                clean_record[key] = str(value)
                        clean_data.append(clean_record)
                    
                    try:
                        import pandas as pd
                        df = pd.DataFrame(clean_data)
                        for col in df.columns:
                            df[col] = df[col].astype(str).replace({'nan': None, 'None': None})
                        
                        self.conn.register('temp_data', df)
                        self.conn.execute("INSERT INTO chicago_311_requests SELECT * FROM temp_data")
                        return len(df)
                    except Exception as e:
                        logger.error(f"DuckDB insert error: {e}")
                        return 0
            
            duckdb_handler = WorkingDuckDBHandler()
            self.handlers['DuckDB'] = duckdb_handler
            logger.info("âœ… DuckDB ready")
        except Exception as e:
            logger.error(f"âŒ DuckDB setup failed: {e}")
        
        logger.info(f"ğŸ¯ Successfully set up {len(self.handlers)} databases: {list(self.handlers.keys())}")
    
    def fetch_data_batch(self, offset: int, limit: int) -> List[Dict]:
        """Fetch a batch from Chicago 311 API."""
        params = {
            '$limit': limit,
            '$offset': offset,
            '$order': ':id'
        }
        
        for attempt in range(self.max_retries):
            try:
                response = requests.get(self.api_url, params=params, timeout=60)
                response.raise_for_status()
                
                data = response.json()
                if not data:
                    return []
                
                return data
                
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed for offset {offset}: {e}")
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(2 ** attempt)
        
        return []
    
    def clean_value_for_all_dbs(self, value: Any) -> Any:
        """Clean value for all database types."""
        if value is None:
            return None
        
        # Handle pandas/numpy types
        if hasattr(value, 'item'):
            return value.item()
        
        if hasattr(value, 'dtype'):
            if hasattr(value, 'item'):
                return value.item()
            else:
                return str(value)
        
        # Handle datetime
        if hasattr(value, 'isoformat'):
            return value.isoformat()
        
        # Handle arrays
        if hasattr(value, 'tolist'):
            return value.tolist()
        
        if str(value) in ['nan', 'NaT', 'None']:
            return None
        
        return value
    
    def transform_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Transform API record for ALL databases."""
        try:
            transformed = {}
            
            # Basic string fields
            string_fields = [
                'sr_number', 'sr_type', 'sr_short_code', 'owner_department', 'status',
                'origin', 'type_of_service_request', 'most_recent_action', 'current_activity',
                'ssa', 'channel', 'street_address', 'city', 'state', 'zip_code',
                'street_number', 'street_direction', 'street_name', 'street_type',
                'duplicate_ssr', 'legacy_sr_number', 'legacy_record', 'parent_sr_number',
                'electrical_district', 'electricity_grid', 'police_sector', 'police_district',
                'police_beat', 'precinct', 'sanitation_division_days', 'created_day_of_week'
            ]
            
            for field in string_fields:
                value = record.get(field)
                if value is None or value == '':
                    transformed[field] = None
                else:
                    transformed[field] = str(value).strip()
            
            # Date fields - use simple ISO format
            date_fields = ['created_date', 'last_modified_date', 'closed_date', 'completion_date', 'due_date']
            for field in date_fields:
                if field in record and record[field]:
                    try:
                        date_str = str(record[field])
                        if 'T' in date_str:
                            transformed[field] = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            try:
                                transformed[field] = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                            except:
                                transformed[field] = datetime.strptime(date_str[:19], '%Y-%m-%dT%H:%M:%S')
                    except:
                        transformed[field] = None
                else:
                    transformed[field] = None
            
            # Numeric fields
            int_fields = ['community_area', 'ward', 'created_hour', 'created_month', 'number_of_days_pending']
            for field in int_fields:
                if field in record and record[field] is not None:
                    try:
                        value = record[field]
                        if str(value) in ['', 'nan', 'None']:
                            transformed[field] = None
                        else:
                            transformed[field] = int(float(value))
                    except (ValueError, TypeError):
                        transformed[field] = None
                else:
                    transformed[field] = None
            
            # Float fields
            float_fields = ['x_coordinate', 'y_coordinate', 'latitude', 'longitude']
            for field in float_fields:
                if field in record and record[field] is not None:
                    try:
                        value = record[field]
                        if str(value) in ['', 'nan', 'None']:
                            transformed[field] = None
                        else:
                            transformed[field] = float(value)
                    except (ValueError, TypeError):
                        transformed[field] = None
                else:
                    transformed[field] = None
            
            # Computed region fields
            computed_fields = {
                'historical_wards_03_15': ':@computed_region_rpca_8um6',
                'zip_codes': ':@computed_region_6mkv_f3dw',
                'community_areas': ':@computed_region_vrxf_vc4k',
                'census_tracts': ':@computed_region_bdys_3d7i',
                'wards': ':@computed_region_43wa_7qmu'
            }
            
            for target_field, source_field in computed_fields.items():
                if source_field in record and record[source_field] is not None:
                    try:
                        value = record[source_field]
                        if str(value) in ['', 'nan', 'None']:
                            transformed[target_field] = None
                        else:
                            transformed[target_field] = int(float(value))
                    except (ValueError, TypeError):
                        transformed[target_field] = None
                else:
                    transformed[target_field] = None
            
            # Handle location data
            if 'location' in record and record['location']:
                location = record['location']
                if isinstance(location, dict) and 'coordinates' in location:
                    coords = location['coordinates']
                    if len(coords) >= 2:
                        transformed['longitude'] = float(coords[0])
                        transformed['latitude'] = float(coords[1])
                        # For Elasticsearch geo_point format
                        transformed['location_point'] = {
                            "lat": float(coords[1]),
                            "lon": float(coords[0])
                        }
                    
            # Create location_point if we have lat/lon
            if not transformed.get('location_point') and transformed.get('latitude') and transformed.get('longitude'):
                transformed['location_point'] = {
                    "lat": transformed['latitude'],
                    "lon": transformed['longitude']
                }
            
            return transformed
            
        except Exception as e:
            logger.error(f"Error transforming record: {e}")
            return {}
    
    def insert_to_database(self, db_name: str, data: List[Dict]) -> int:
        """Insert data into specific database with ALL FIXES."""
        if not data or db_name not in self.handlers:
            return 0
        
        try:
            handler = self.handlers[db_name]
            
            if db_name == 'MongoDB':
                try:
                    clean_data = []
                    for doc in data:
                        clean_doc = {}
                        for key, value in doc.items():
                            clean_doc[key] = self.clean_value_for_all_dbs(value)
                        clean_data.append(clean_doc)
                    
                    result = handler.collection.insert_many(clean_data, ordered=False)
                    return len(result.inserted_ids)
                except Exception as e:
                    logger.error(f"MongoDB insert error: {e}")
                    return 0
            
            elif db_name == 'Elasticsearch':
                try:
                    from elasticsearch.helpers import bulk
                    
                    actions = []
                    for doc in data:
                        clean_doc = {}
                        for key, value in doc.items():
                            cleaned_value = self.clean_value_for_all_dbs(value)
                            
                            # Final cleaning for Elasticsearch
                            if cleaned_value is None:
                                clean_doc[key] = None
                            elif isinstance(cleaned_value, (int, float, str, bool, list, dict)):
                                clean_doc[key] = cleaned_value
                            elif hasattr(cleaned_value, 'isoformat'):
                                clean_doc[key] = cleaned_value.isoformat()
                            else:
                                clean_doc[key] = str(cleaned_value)
                        
                        actions.append({
                            "_index": handler.index_name,
                            "_source": clean_doc
                        })
                    
                    success_count, _ = bulk(handler.es, actions, request_timeout=120)
                    return success_count
                except Exception as e:
                    logger.error(f"Elasticsearch insert error: {e}")
                    return 0
            
            else:  # PostgreSQL or DuckDB
                clean_data = []
                for doc in data:
                    clean_doc = {}
                    for key, value in doc.items():
                        clean_doc[key] = self.clean_value_for_all_dbs(value)
                    clean_data.append(clean_doc)
                
                return handler.bulk_insert(clean_data)
            
        except Exception as e:
            logger.error(f"âŒ {db_name} insert failed: {e}")
            return 0
    
    def process_batch(self, offset: int) -> Dict[str, int]:
        """Process a single batch across all databases."""
        try:
            # Fetch data
            raw_data = self.fetch_data_batch(offset, self.batch_size)
            if not raw_data:
                return {}
            
            # Transform data
            transformed_data = []
            for record in raw_data:
                transformed = self.transform_record(record)
                if transformed:
                    transformed_data.append(transformed)
            
            if not transformed_data:
                return {}
            
            # Insert into all databases concurrently
            results = {}
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = {
                    executor.submit(self.insert_to_database, db_name, transformed_data): db_name
                    for db_name in self.handlers.keys()
                }
                
                for future in as_completed(futures):
                    db_name = futures[future]
                    try:
                        inserted_count = future.result()
                        results[db_name] = inserted_count
                        if inserted_count > 0:
                            logger.info(f"âœ… {db_name}: {inserted_count} records")
                        else:
                            logger.warning(f"âš ï¸  {db_name}: 0 records")
                    except Exception as e:
                        logger.error(f"âŒ {db_name}: {e}")
                        results[db_name] = 0
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing batch at offset {offset}: {e}")
            return {}
    
    def run_complete_loading(self):
        """Load complete 12.4M dataset into ALL 4 databases."""
        target_records = 12400000
        logger.info(f"ğŸš€ FINAL COMPLETE - ALL 4 DATABASES LOADING 12.4M RECORDS!")
        logger.info(f"ğŸ“Š Target: {target_records:,} records")
        
        # Setup databases
        self.setup_databases()
        if len(self.handlers) < 4:
            logger.warning(f"âš ï¸ Only {len(self.handlers)} databases ready! Target is 4!")
            logger.info(f"Available: {list(self.handlers.keys())}")
        
        self.start_time = time.time()
        
        # Progress tracking
        last_progress_time = time.time()
        progress_interval = 60  # Report every minute
        
        offset = 0
        db_totals = {db_name: 0 for db_name in self.handlers.keys()}
        consecutive_failures = 0
        
        try:
            while offset < target_records:
                # Process batch
                batch_results = self.process_batch(offset)
                
                if not batch_results or all(count == 0 for count in batch_results.values()):
                    consecutive_failures += 1
                    logger.warning(f"âŒ No data or all failed (failure #{consecutive_failures})")
                    if consecutive_failures >= 5:
                        logger.error("âŒ Too many consecutive failures, stopping...")
                        break
                else:
                    consecutive_failures = 0
                
                # Update totals
                for db_name, count in batch_results.items():
                    db_totals[db_name] += count
                
                self.total_processed += self.batch_size
                offset += self.batch_size
                
                # Progress reporting
                current_time = time.time()
                if current_time - last_progress_time >= progress_interval:
                    elapsed = current_time - self.start_time
                    rate = self.total_processed / elapsed if elapsed > 0 else 0
                    eta_seconds = (target_records - self.total_processed) / rate if rate > 0 else 0
                    eta = timedelta(seconds=int(eta_seconds))
                    
                    progress_pct = (self.total_processed / target_records) * 100
                    
                    logger.info("=" * 80)
                    logger.info(f"ğŸ“Š PROGRESS: {self.total_processed:,}/{target_records:,} ({progress_pct:.1f}%)")
                    logger.info(f"âš¡ Rate: {rate:.0f} records/sec | ETA: {eta}")
                    logger.info(f"ğŸ’¾ Database totals:")
                    total_loaded = 0
                    working_databases = 0
                    for db_name, count in db_totals.items():
                        if count > 0:
                            logger.info(f"   âœ… {db_name}: {count:,} records")
                            working_databases += 1
                        else:
                            logger.info(f"   âŒ {db_name}: {count:,} records")
                        total_loaded += count
                    logger.info(f"ğŸ¯ Total across all DBs: {total_loaded:,}")
                    logger.info(f"ğŸ”¥ Working databases: {working_databases}/4")
                    logger.info("=" * 80)
                    
                    last_progress_time = current_time
                
                # Adaptive delay
                time.sleep(self.delay_between_batches)
                
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ Loading interrupted by user")
        except Exception as e:
            logger.error(f"âŒ Loading failed: {e}")
            traceback.print_exc()
        
        # Final summary
        total_time = time.time() - self.start_time
        avg_rate = self.total_processed / total_time if total_time > 0 else 0
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ FINAL COMPLETE LOADING FINISHED!")
        logger.info(f"â±ï¸  Total time: {timedelta(seconds=int(total_time))}")
        logger.info(f"ğŸ“Š Total processed: {self.total_processed:,} records")
        logger.info(f"âš¡ Average rate: {avg_rate:.0f} records/sec")
        logger.info("ğŸ’¾ Final database counts:")
        
        total_loaded = 0
        successful_dbs = 0
        for db_name, count in db_totals.items():
            if count > 0:
                logger.info(f"   âœ… {db_name}: {count:,} records")
                successful_dbs += 1
            else:
                logger.info(f"   âŒ {db_name}: {count:,} records")
            total_loaded += count
        
        logger.info(f"ğŸ¯ Total loaded across ALL databases: {total_loaded:,}")
        logger.info(f"ğŸ”¥ Successful databases: {successful_dbs}/4")
        
        # Check if ALL 4 databases have data
        if successful_dbs == 4:
            logger.info("ğŸ‰ğŸ‰ğŸ‰ SUCCESS! ALL 4 DATABASES loaded successfully!")
            logger.info("ğŸš€ğŸš€ READY FOR COMPREHENSIVE BENCHMARKING AND STRESS TESTING!")
        else:
            logger.warning(f"âš ï¸ Only {successful_dbs} out of 4 databases loaded successfully")
        
        logger.info("=" * 80)
        
        return db_totals

def main():
    """Main execution."""
    try:
        loader = FinalCompleteAll4DatabasesLoader()
        results = loader.run_complete_loading()
        
        # Check if ready for benchmarking
        successful_dbs = [db for db, count in results.items() if count > 0]
        if len(successful_dbs) == 4:
            logger.info("ğŸš€ğŸš€ğŸš€ ALL 4 DATABASES WITH 12.4M RECORDS - READY FOR BENCHMARKING!")
        else:
            logger.info(f"ğŸš€ {len(successful_dbs)} databases ready for benchmarking")
        
        return results
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Loading interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Loading failed: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()